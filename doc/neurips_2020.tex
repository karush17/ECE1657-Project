\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint,nonatbib]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
    \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
    %  \usepackage[nonatbib]{neurips_2020}
\usepackage{flushend}
\usepackage{enumitem}
\usepackage{bookmark}
\usepackage{graphics}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amsfonts}
\usepackage{algorithm,algorithmicx}
\usepackage{algpseudocode}
\usepackage{microtype}      % microtypography
\usepackage{pdfpages}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage[toc,page]{appendix}
\usepackage{balance} % for balancing columns on the final page
\usepackage{caption} 
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
   \renewcommand\customgenericname{#2}%
   \renewcommand\theinnercustomgeneric{##1}%
   \innercustomgeneric
  }
  {\endinnercustomgeneric}
}

\newcustomtheorem{customthm}{Theorem}
\newcustomtheorem{customlemma}{Lemma}

\title{Cooperation in Multi-Agent Reinforcement Learning}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
  Karush Suri, Dian Gadjov, Lacra Pavel\\
   Department of Electrical \& Computer Engineering, University of Toronto, Canada.\\
  \texttt{karush.suri@mail.utoronto.ca}
}


\begin{document}

\maketitle

\begin{abstract}
Advancements in Multi-Agent Reinforcement Learning (MARL) are motivated by cooperation in agents arising from Game Theory (GT). Agents must collaborate in practical scenarios in order to achieve complex objectives and attain strategies which depict optimal behavior. The need for cooperation is further highlighted in the case of partially-observed settings wherein agents have restricted access to environment observations. We revisit cooperation in MARL from the viewpoint of GT and stochastic dynamics of environments. The contributions of our work are twofold. (1) We analyze and demonstrate the effectiveness of cooperative MARL in the case of complex and partially-observed tasks consisting of high-dimensional action spaces and stochastic dynamics. (2) We leverage the empirical demonstrations to construct a novel optimization objective which addresses the detrimental effects of spurious states across agents. Our large-scale experiments carried out on the StarCraft II benchmark depict the effectiveness of cooperative MARL and our novel objective for obtaining optimal strategies under stochastic dynamics. 

\end{abstract}

\begin{multicols}{2}

\section{Introduction}
Reinforcement Learning (RL) has seen tremendous growth in applications such as arcade games \cite{atari}, board games \cite{go, shogi}, robot control tasks \cite{ddpg, ppo} and lately, real-time games \cite{SC2}. The rise of RL has led to an increasing interest in the study of multi-agent systems \cite{maddpg, alphastar}, commonly known as Multi-Agent Reinforcement Learning (MARL). MARL provides significant benefits in comparison to contemporary single-agent methods \cite{rl}. The Multi-Agent framework allows the modelling of complex real-world systems which consist of dynamic and large-scale interactions between multiple agents \cite{ltc}. Additionally, MARL enables the learning of diverse strategies which are essential for executing a range of different tasks by the same set of agents. 

In the case of partially observable settings, MARL enables the learning of strategies from a GT perspective by utilizing cooperation across agents\cite{cooperative}. Agents collaborate with each other in a given environment to optimize the cumulative payoffs by means of a single utility function. Optimization of the joint utility function leads to optimal behavior \cite{gt,selective} in the long-horizon which is characterized by each agent executing its optimal strategy irrespective of other agents. Such a framework of learning strategies with collaborators and executing behaviors independently is often referred to as centralized training with decentralized control \cite{coma}.

The regime of decentralized control is hindered by intrinsic stochasticity in the environment. Spurious states are a common phenomenon observed in the case of single-agent RL methods. In the case of model-based RL \cite{mbrl}, agents build a model of the environment which learns the dynamics of the environment. Such a scheme is used as an effective planning tool in the case of long-horizon tasks \cite{smirl}. In the case of model-free RL methods, environment stochasticity is addressed by utilizing robust utility functions \cite{surprise,surpmodeling} and effective exploration strategies \cite{gen}. On the other hand, MARL does not account for spurious states across agents as a result of which the system remains unaware of drastic changes in the environment \cite{role}. Thus, addressing the learning of stochastic dynamics in the case of multi-agent settings requires attention from a critical standpoint.

We revisit cooperation in MARL from the perspective of GT and stochastic dynamics in the agents' environment. Our work assesses and demonstrates collaborative schemes in MARL under partially-observed settings which pose ill-conditioned objectives for the multi-agent system. More specifically, our twofold contributions are the following- 

\begin{itemize}[leftmargin=*]
\item We analyze and demonstrate the effectiveness of cooperative MARL for complex and partially-observed tasks consisting of high-dimensional action spaces and spurious states.
\item We leverage the empirical demonstrations to construct a novel optimization objective which addresses the detrimental effects of spurious states across agents. 
\end{itemize}

Our large-scale experiments carried out on the StarCraft II benchmark depict the effectiveness of cooperative MARL and our novel objective for obtaining optimal strategies under stochastic dynamics.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
Growing advances in GT have given rise to efficient MARL algorithms and implementations in stochastic scenarios. Thsi section highlights some of the main contributions in learning of stochastic games which have paved the way for Multi-Agent learning. 

\subsection{Learning in Games}
% lit review of GT (must cover everything)
dfvd

\subsection{Multi-Agent Learning}
Most multi-agent methods are based on the paradigm of centralized training and decentralized control \cite{coma,vdn,qmix} wherein agents learn to collaborate \cite{coordinate} and optimize their utility function \cite{coordinatedrl}. The fundamental work on MARL originates from the IQL \cite{iql} framework wherein agents learn to collaborate with independent utilities. While the IQL framework serves as a critical point for advances in MARL, the work of \cite{jakob} presents the common knowledge framework wherein agents collaborate by gaining mutual information about the task and establishing a structured protocol for communication \cite{comm}. Such methods have given rise to large-scale agents capable of optimal behavior on high-dimensional control tasks \cite{maddpg,aac,rmaddpg}. Some of these methods suffer from estimation biases \cite{marlover,iqn} stemming from the function approximator \cite{double} used to maximize the utility function. Various MARL methods \cite{twinmix} make use of a dual function approximator approach which increases the accuracy of estimates. Another suitable approach is the usage of weighted bellman updates in double Q-learning \cite{weightdouble}. The Weighted Double Deep $Q$-Network (WDDQN) provides stability and sample efficiency for fully-observable settings. In the case of partially-observed scenarios, Weighted-QMIX (WQMIX) \cite{wqmix} yields a more sophisticated weighting scheme which aids in the retrieval of optimal strategy \cite{challenges}.

Despite the recent success of RL \cite{a3c,rainbow} MARL agents suffer from spurious state spaces and encounter sudden changes in trajectories. These anomalous transitions between consecutive states are often termed as surprise \cite{surprise}. Quantitatively, surprise can be inferred as a measure of deviation \cite{smirl,gen} among states encountered by the agent during its interaction with the environment. In the case of single-agent methods, surprise results in sample-inefficient learning \cite{surprise}. This can be tackled by making use of rigorous exploration strategies \cite{effectiveexp,statemarginal}. However, such solutions do not show evidence for multiple agents consisting of individual partial observations \cite{marlsurp}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminaries}

\subsection{Stochastic Markov Games}
% explain markov games and their details

\subsection{Multi-Agent Learning}
% explain MARL in detail

\subsection{Q-Learning}
% explain Q-learning in detail

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Cooperation in Multi-Agent Learning}
% talk about cooperation, implementation and analyze

\subsection{The Partial Observability Setting}
% explain the partial observability setting in detail

\subsection{Learning Model-Free Behaviors}
% expand out each algorithm with its details

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tackling Spurious Dynamics}
% novelty comes here, talk about surprise 
% talk about model-based and model-free methods
% highlght the math- ATLEAST 2 NOVEL CONTRIBUTIONS!!
% do no dive too much into RL, think from a GT perspective


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Experiments}

\subsection{The StarCraft II Benchmark}
% explain out the SC2 benchmark, reasons and its highlights

\subsection{Performance}
% your RL results and details come here

\subsection{Spurious Dynamics}
% your novel results and explaination comes here

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}


\bibliographystyle{unsrt} 
\footnotesize{\bibliography{sample}}
\end{multicols}

% THINGS TO INCLUDE IN THE SUPPLEMENTARY-
% APPENDIX- additional results, implementation details
% VIDEOS- videos of all agents
% CODE- codebase for simulation

\end{document}
