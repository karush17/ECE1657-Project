\begin{thebibliography}{10}

\bibitem{atari}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em CoRR}, abs/1312.5602, 2013.

\bibitem{go}
David Silver, Aja Huang, Chris~J. Maddison, Arthur Guez, Laurent Sifre, George
  van~den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal
  Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray
  Kavukcuoglu, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock {\em Nature}, 529(7587):484--489, January 2016.

\bibitem{shogi}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, Timothy Lillicrap, and David Silver.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model, 2019.

\bibitem{ddpg}
Timothy~P. Lillicrap, Jonathan~J. Hunt, Alexander Pritzel, Nicolas Manfred~Otto
  Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em CoRR}, abs/1509.02971, 2015.

\bibitem{ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em CoRR}, abs/1707.06347, 2017.

\bibitem{SC2}
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander~Sasha
  Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou,
  Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen
  Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy Lillicrap,
  Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo,
  Jacob Repp, and Rodney Tsing.
\newblock Starcraft ii: A new challenge for reinforcement learning, 2017.

\bibitem{maddpg}
Ryan Lowe, Yi~Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments, 2017.

\bibitem{alphastar}
Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michaël Mathieu, Andrew
  Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko
  Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang,
  Laurent Sifre, Trevor Cai, John Agapiou, Max Jaderberg, and David Silver.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575, 11 2019.

\bibitem{dec}
Landon Kraemer and Bikramjit Banerjee.
\newblock Multi-agent reinforcement learning as a rehearsal for decentralized
  planning.
\newblock {\em Neurocomputing}, 190, 02 2016.

\bibitem{iql}
Ming Tan.
\newblock Multi-agent reinforcement learning: Independent vs. cooperative
  agents.
\newblock In {\em In Proceedings of the Tenth International Conference on
  Machine Learning}, 1993.

\bibitem{coma}
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and
  Shimon Whiteson.
\newblock Counterfactual multi-agent policy gradients, 2017.

\bibitem{vdn}
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech~Marian Czarnecki, Vinicius
  Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel~Z. Leibo, Karl
  Tuyls, and Thore Graepel.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In {\em Proceedings of the 17th International Conference on
  Autonomous Agents and MultiAgent Systems}, AAMAS ’18, page 2085–2087,
  2018.

\bibitem{qmix}
Tabish Rashid, Mikayel Samvelyan, Christian~Schroeder de~Witt, Gregory
  Farquhar, Jakob Foerster, and Shimon Whiteson.
\newblock Qmix: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In {\em ICML 2018: Proceedings of the Thirty-Fifth International
  Conference on Machine Learning}, 2018.

\bibitem{doubleqlearning}
Hado~V. Hasselt.
\newblock Double q-learning.
\newblock In {\em Advances in Neural Information Processing Systems 23}. 2010.

\bibitem{deepdoubleqlearning}
Hado~van Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In {\em Proceedings of the Thirtieth AAAI Conference on Artificial
  Intelligence}, 2016.

\bibitem{td3}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods,
  2018.

\bibitem{maxmin}
Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White.
\newblock Maxmin q-learning: Controlling the estimation bias of q-learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{surprise}
Joshua Achiam and Shankar Sastry.
\newblock Surprise-based intrinsic motivation for deep reinforcement learning,
  2017.

\bibitem{smirl}
Glen Berseth, Daniel Geng, Coline Devin, Dinesh Jayaraman, Chelsea Finn, and
  Sergey Levine.
\newblock Smirl: Surprise minimizing rl in entropic environments.
\newblock 2019.

\bibitem{mbrl}
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy~H
  Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski,
  Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, and Henryk
  Michalewski.
\newblock Model-based reinforcement learning for atari, 2019.

\bibitem{surpmodeling}
Luis Macedo, Rainer Reisezein, and Amilcar Cardoso.
\newblock Modeling forms of surprise in artificial agents: empirical and
  theoretical study of surprise functions.
\newblock In {\em Proceedings of the Annual Meeting of the Cognitive Science
  Society}, volume~26, 2004.

\bibitem{gen}
Jerry~Zikun Chen.
\newblock Reinforcement learning generalization with surprise minimization,
  2020.

\bibitem{role}
Luis Macedo and Amilcar Cardoso.
\newblock The role of surprise, curiosity and hunger on exploration of unknown
  environments populated with entities.
\newblock In {\em 2005 portuguese conference on artificial intelligence}, 2005.

\bibitem{smac}
Mikayel Samvelyan, Tabish Rashid, Christian~Schroeder de~Witt, Gregory
  Farquhar, Nantas Nardelli, Tim G.~J. Rudner, Chia-Man Hung, Philip H.~S.
  Torr, Jakob Foerster, and Shimon Whiteson.
\newblock The starcraft multi-agent challenge, 2019.

\bibitem{rl}
Richard~S. Sutton and Andrew~G. Barto.
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock 2018.

\bibitem{nash}
John~F. Nash.
\newblock Equilibrium points in n-person games.
\newblock {\em Proceedings of the National Academy of Sciences}, 36(1), 1950.

\bibitem{a3c}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, 2016.

\bibitem{rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1710.02298}, 2017.

\bibitem{curiosity}
Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and
  Alexei~A. Efros.
\newblock Large-scale study of curiosity-driven learning.
\newblock In {\em ICLR}, 2019.

\bibitem{exploration}
Sebastian~B Thrun.
\newblock Efficient exploration in reinforcement learning.
\newblock 1992.

\bibitem{effectiveexp}
Bradly~C Stadie, Sergey Levine, and Pieter Abbeel.
\newblock Incentivizing exploration in reinforcement learning with deep
  predictive models.
\newblock {\em arXiv preprint arXiv:1507.00814}, 2015.

\bibitem{statemarginal}
Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and
  Ruslan Salakhutdinov.
\newblock Efficient exploration via state marginal matching.
\newblock {\em arXiv preprint arXiv:1906.05274}, 2019.

\bibitem{hdqn}
Tejas~D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation.
\newblock In {\em Advances in neural information processing systems}, 2016.

\bibitem{metaexp}
Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.
\newblock Meta-reinforcement learning of structured exploration strategies.
\newblock In {\em Advances in Neural Information Processing Systems 31}. 2018.

\bibitem{marlsurp}
Wei Ren, Randal~W Beard, and Ella~M Atkins.
\newblock A survey of consensus problems in multi-agent coordination.
\newblock In {\em Proceedings of the 2005, American Control Conference, 2005.},
  2005.

\bibitem{duel}
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando
  Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, 2016.

\bibitem{sac}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}, 2018.

\bibitem{marlover}
Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama.
\newblock Reducing overestimation bias in multi-agent domains using double
  centralized critics.
\newblock {\em arXiv preprint arXiv:1910.01465}, 2019.

\bibitem{iqn}
Xueguang Lyu and Christopher Amato.
\newblock Likelihood quantile networks for coordinating multi-agent
  reinforcement learning.
\newblock In {\em Proceedings of the 19th International Conference on
  Autonomous Agents and MultiAgent Systems}, 2020.

\bibitem{twinmix}
Zipeng Fu, Qingqing Zhao, and Weinan Zhang.
\newblock Reducing overestimation in value mixing for cooperative deep
  multi-agent reinforcement learning.
\newblock {\em ICAART}, 2020.

\bibitem{weightdouble}
Yan Zheng, Zhaopeng Meng, Jianye Hao, and Zongzhang Zhang.
\newblock Weighted double deep multiagent reinforcement learning in stochastic
  cooperative environments.
\newblock In {\em Pacific Rim international conference on artificial
  intelligence}, 2018.

\bibitem{wqmix}
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson.
\newblock Weighted qmix: Expanding monotonic value function factorisation,
  2020.

\bibitem{challenges}
Thanh~Thi Nguyen, Ngoc~Duy Nguyen, and Saeid Nahavandi.
\newblock Deep reinforcement learning for multiagent systems: A review of
  challenges, solutions, and applications.
\newblock {\em IEEE transactions on cybernetics}, 2020.

\bibitem{ebm}
Yann LeCun, Sumit Chopra, Raia Hadsell, M~Ranzato, and F~Huang.
\newblock A tutorial on energy-based learning.
\newblock {\em Predicting structured data}, 1, 2006.

\bibitem{ebmdoc}
Yann LeCun, Sumit Chopra, M~Ranzato, and F-J Huang.
\newblock Energy-based models in document recognition and computer vision.
\newblock In {\em Ninth International Conference on Document Analysis and
  Recognition (ICDAR 2007)}, volume~1, 2007.

\bibitem{energy}
Yee~Whye Teh, Max Welling, Simon Osindero, and Geoffrey~E Hinton.
\newblock Energy-based models for sparse overcomplete representations.
\newblock {\em Journal of Machine Learning Research}, 4, 2003.

\bibitem{david}
David J.~C. MacKay.
\newblock {\em Information Theory, Inference \& Learning Algorithms}.
\newblock Cambridge University Press, 2002.

\bibitem{rlhinton}
Brian Sallans and Geoffrey~E Hinton.
\newblock Reinforcement learning with factored states and actions.
\newblock {\em Journal of Machine Learning Research}, 5, 2004.

\bibitem{boltzmann}
Sergey Levine and Pieter Abbeel.
\newblock Learning neural network policies with guided policy search under
  unknown dynamics.
\newblock In {\em Advances in Neural Information Processing Systems}, 2014.

\bibitem{pgq}
Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih.
\newblock Combining policy gradient and q-learning.
\newblock {\em arXiv preprint arXiv:1611.01626}, 2016.

\bibitem{sql}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock {\em arXiv preprint arXiv:1702.08165}, 2017.

\bibitem{inference}
Marc Toussaint.
\newblock Robot trajectory optimization using approximate inference.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, 2009.

\bibitem{ziebartinverse}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, and Anind~K Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In {\em AAAI}, 2008.

\bibitem{ziebartphd}
Brian~D Ziebart.
\newblock Modeling purposeful adaptive behavior with the principle of maximum
  causal entropy.
\newblock 2010.

\bibitem{equivalence}
John Schulman, Xi~Chen, and Pieter Abbeel.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock {\em arXiv preprint arXiv:1704.06440}, 2017.

\bibitem{haarnoja}
Tuomas Haarnoja.
\newblock {\em Acquiring Diverse Robot Skills via Maximum Entropy Deep
  Reinforcement Learning}.
\newblock PhD thesis, UC Berkeley, 2018.

\bibitem{hierarchical}
Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine.
\newblock Latent space policies for hierarchical reinforcement learning.
\newblock {\em arXiv preprint arXiv:1804.02808}, 2018.

\bibitem{overview}
Lucian Bu{\c{s}}oniu, Robert Babu{\v{s}}ka, and Bart De~Schutter.
\newblock Multi-agent reinforcement learning: An overview.
\newblock In {\em Innovations in multi-agent systems and applications-1}. 2010.

\bibitem{probabilistic}
Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan.
\newblock Probabilistic recursive reasoning for multi-agent reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1901.09207}, 2019.

\bibitem{twoplayer}
Jordi Grau-Moya, Felix Leibfried, and Haitham Bou-Ammar.
\newblock Balancing two-player stochastic games with soft q-learning.
\newblock {\em arXiv preprint arXiv:1802.03216}, 2018.

\bibitem{marlsql}
Ermo Wei, Drew Wicke, David Freelan, and Sean Luke.
\newblock Multiagent soft q-learning.
\newblock {\em arXiv preprint arXiv:1804.09817}, 2018.

\bibitem{mellowmax}
Kavosh Asadi and Michael~L Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{homer}
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford.
\newblock Kinematic state abstraction and provably efficient rich-observation
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.05815}, 2019.

\end{thebibliography}
