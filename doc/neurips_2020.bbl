\begin{thebibliography}{10}

\bibitem{atari}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em CoRR}, abs/1312.5602, 2013.

\bibitem{go}
David Silver, Aja Huang, Chris~J. Maddison, Arthur Guez, Laurent Sifre, George
  van~den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal
  Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray
  Kavukcuoglu, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock {\em Nature}, 529(7587):484--489, January 2016.

\bibitem{shogi}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, Timothy Lillicrap, and David Silver.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model, 2019.

\bibitem{ddpg}
Timothy~P. Lillicrap, Jonathan~J. Hunt, Alexander Pritzel, Nicolas Manfred~Otto
  Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em CoRR}, abs/1509.02971, 2015.

\bibitem{ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em CoRR}, abs/1707.06347, 2017.

\bibitem{SC2}
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander~Sasha
  Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou,
  Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen
  Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy Lillicrap,
  Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo,
  Jacob Repp, and Rodney Tsing.
\newblock Starcraft ii: A new challenge for reinforcement learning, 2017.

\bibitem{maddpg}
Ryan Lowe, Yi~I Wu, Aviv Tamar, Jean Harb, OpenAI~Pieter Abbeel, and Igor
  Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock In {\em Advances in neural information processing systems}, pages
  6379--6390, 2017.

\bibitem{alphastar}
Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michaël Mathieu, Andrew
  Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko
  Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang,
  Laurent Sifre, Trevor Cai, John Agapiou, Max Jaderberg, and David Silver.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575, 11 2019.

\bibitem{rl}
Richard~S. Sutton and Andrew~G. Barto.
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock 2018.

\bibitem{ltc}
Gon{\c{c}}alo Neto.
\newblock From single-agent to multi-agent reinforcement learning: Foundational
  concepts and methods.
\newblock {\em Learning theory course}, 2005.

\bibitem{cooperative}
Liviu Panait and Sean Luke.
\newblock Cooperative multi-agent learning: The state of the art.
\newblock {\em Autonomous agents and multi-agent systems}, 11(3):387--434,
  2005.

\bibitem{gt}
Ann Now{\'e}, Peter Vrancx, and Yann-Micha{\"e}l De~Hauwere.
\newblock Game theory and multi-agent reinforcement learning.
\newblock In {\em Reinforcement Learning}, pages 441--470. Springer, 2012.

\bibitem{selective}
Kaiqing Zhang, Zhuoran Yang, and Tamer Ba{\c{s}}ar.
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock {\em arXiv preprint arXiv:1911.10635}, 2019.

\bibitem{coma}
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and
  Shimon Whiteson.
\newblock Counterfactual multi-agent policy gradients, 2017.

\bibitem{mbrl}
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy~H
  Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski,
  Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, and Henryk
  Michalewski.
\newblock Model-based reinforcement learning for atari, 2019.

\bibitem{smirl}
Glen Berseth, Daniel Geng, Coline Devin, Dinesh Jayaraman, Chelsea Finn, and
  Sergey Levine.
\newblock Smirl: Surprise minimizing rl in entropic environments.
\newblock 2019.

\bibitem{surprise}
Joshua Achiam and Shankar Sastry.
\newblock Surprise-based intrinsic motivation for deep reinforcement learning,
  2017.

\bibitem{surpmodeling}
Luis Macedo, Rainer Reisezein, and Amilcar Cardoso.
\newblock Modeling forms of surprise in artificial agents: empirical and
  theoretical study of surprise functions.
\newblock In {\em Proceedings of the Annual Meeting of the Cognitive Science
  Society}, volume~26, 2004.

\bibitem{gen}
Jerry~Zikun Chen.
\newblock Reinforcement learning generalization with surprise minimization,
  2020.

\bibitem{role}
Luis Macedo and Amilcar Cardoso.
\newblock The role of surprise, curiosity and hunger on exploration of unknown
  environments populated with entities.
\newblock In {\em 2005 portuguese conference on artificial intelligence}, 2005.

\bibitem{learningingames}
Drew Fudenberg and David Levine.
\newblock {\em The Theory of Learning in Games}, volume~1.
\newblock The MIT Press, 1 edition, 1998.

\bibitem{benaim}
Michel Benaim and Morris~W Hirsch.
\newblock Learning processes, mixed equilibria and dynamical systems.
\newblock {\em Games and Economic Behavior}, 29:36--72, 1999.

\bibitem{cnn}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,
  editors, {\em Advances in Neural Information Processing Systems 25}, pages
  1097--1105. 2012.

\bibitem{ale}
Marc~G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em J. Artif. Int. Res.}, page 253–279, 2013.

\bibitem{doubleqlearning}
Hado~V. Hasselt.
\newblock Double q-learning.
\newblock In {\em Advances in Neural Information Processing Systems 23}. 2010.

\bibitem{exploration}
Sebastian~B Thrun.
\newblock Efficient exploration in reinforcement learning.
\newblock 1992.

\bibitem{deepdoubleqlearning}
Hado~van Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In {\em Proceedings of the Thirtieth AAAI Conference on Artificial
  Intelligence}, 2016.

\bibitem{a3c}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, 2016.

\bibitem{haarnoja}
Tuomas Haarnoja.
\newblock {\em Acquiring Diverse Robot Skills via Maximum Entropy Deep
  Reinforcement Learning}.
\newblock PhD thesis, UC Berkeley, 2018.

\bibitem{vdn}
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech~Marian Czarnecki, Vinicius
  Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel~Z. Leibo, Karl
  Tuyls, and Thore Graepel.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In {\em Proceedings of the 17th International Conference on
  Autonomous Agents and MultiAgent Systems}, AAMAS ’18, page 2085–2087,
  2018.

\bibitem{qmix}
Tabish Rashid, Mikayel Samvelyan, Christian~Schroeder de~Witt, Gregory
  Farquhar, Jakob Foerster, and Shimon Whiteson.
\newblock Qmix: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In {\em ICML 2018: Proceedings of the Thirty-Fifth International
  Conference on Machine Learning}, 2018.

\bibitem{coordinate}
Jianye Hao, Dongping Huang, Yi~Cai, and Ho-Fung Leung.
\newblock Reinforcement social learning of coordination in networked
  cooperative multiagent systems.
\newblock In {\em AAAI workshop on multiagent interaction without prior
  coordination (MIPC 2014)}, 2014.

\bibitem{coordinatedrl}
Carlos Guestrin, Michail Lagoudakis, and Ronald Parr.
\newblock Coordinated reinforcement learning.
\newblock In {\em ICML}, volume~2, pages 227--234, 2002.

\bibitem{iql}
Ming Tan.
\newblock Multi-agent reinforcement learning: Independent vs. cooperative
  agents.
\newblock In {\em In Proceedings of the Tenth International Conference on
  Machine Learning}, 1993.

\bibitem{jakob}
Jakob~N Foerster.
\newblock {\em Deep multi-agent reinforcement learning}.
\newblock PhD thesis, University of Oxford, 2018.

\bibitem{comm}
Jakob Foerster, Ioannis~Alexandros Assael, Nando De~Freitas, and Shimon
  Whiteson.
\newblock Learning to communicate with deep multi-agent reinforcement learning.
\newblock In {\em Advances in neural information processing systems}, pages
  2137--2145, 2016.

\bibitem{aac}
Shariq Iqbal and Fei Sha.
\newblock Actor-attention-critic for multi-agent reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2961--2970. PMLR, 2019.

\bibitem{rmaddpg}
Rose~E Wang, Michael Everett, and Jonathan~P How.
\newblock R-maddpg for partially observable environments and limited
  communication.
\newblock {\em arXiv preprint arXiv:2002.06684}, 2020.

\bibitem{marlover}
Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama.
\newblock Reducing overestimation bias in multi-agent domains using double
  centralized critics.
\newblock {\em arXiv preprint arXiv:1910.01465}, 2019.

\bibitem{iqn}
Xueguang Lyu and Christopher Amato.
\newblock Likelihood quantile networks for coordinating multi-agent
  reinforcement learning.
\newblock In {\em Proceedings of the 19th International Conference on
  Autonomous Agents and MultiAgent Systems}, 2020.

\bibitem{double}
Hado~V Hasselt.
\newblock Double q-learning.
\newblock In {\em Advances in neural information processing systems}, pages
  2613--2621, 2010.

\bibitem{twinmix}
Zipeng Fu, Qingqing Zhao, and Weinan Zhang.
\newblock Reducing overestimation in value mixing for cooperative deep
  multi-agent reinforcement learning.
\newblock {\em ICAART}, 2020.

\bibitem{wqmix}
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson.
\newblock Weighted qmix: Expanding monotonic value function factorisation,
  2020.

\bibitem{challenges}
Thanh~Thi Nguyen, Ngoc~Duy Nguyen, and Saeid Nahavandi.
\newblock Deep reinforcement learning for multiagent systems: A review of
  challenges, solutions, and applications.
\newblock {\em IEEE transactions on cybernetics}, 2020.

\bibitem{rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1710.02298}, 2017.

\bibitem{effectiveexp}
Bradly~C Stadie, Sergey Levine, and Pieter Abbeel.
\newblock Incentivizing exploration in reinforcement learning with deep
  predictive models.
\newblock {\em arXiv preprint arXiv:1507.00814}, 2015.

\bibitem{statemarginal}
Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and
  Ruslan Salakhutdinov.
\newblock Efficient exploration via state marginal matching.
\newblock {\em arXiv preprint arXiv:1906.05274}, 2019.

\bibitem{marlsurp}
Wei Ren, Randal~W Beard, and Ella~M Atkins.
\newblock A survey of consensus problems in multi-agent coordination.
\newblock In {\em Proceedings of the 2005, American Control Conference, 2005.},
  2005.

\bibitem{overview}
Lucian Bu{\c{s}}oniu, Robert Babu{\v{s}}ka, and Bart De~Schutter.
\newblock Multi-agent reinforcement learning: An overview.
\newblock In {\em Innovations in multi-agent systems and applications-1}. 2010.

\bibitem{nash}
John~F. Nash.
\newblock Equilibrium points in n-person games.
\newblock {\em Proceedings of the National Academy of Sciences}, 36(1), 1950.

\bibitem{smac}
Mikayel Samvelyan, Tabish Rashid, Christian~Schroeder de~Witt, Gregory
  Farquhar, Nantas Nardelli, Tim G.~J. Rudner, Chia-Man Hung, Philip H.~S.
  Torr, Jakob Foerster, and Shimon Whiteson.
\newblock The starcraft multi-agent challenge, 2019.

\bibitem{mellowmax}
Kavosh Asadi and Michael~L Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{sql}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock {\em arXiv preprint arXiv:1702.08165}, 2017.

\bibitem{curiosity}
Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and
  Alexei~A. Efros.
\newblock Large-scale study of curiosity-driven learning.
\newblock In {\em ICLR}, 2019.

\end{thebibliography}
