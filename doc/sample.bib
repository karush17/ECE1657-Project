@article{evolutionary,
  title={Evolutionary dynamics of multi-agent learning: A survey},
  author={Bloembergen, Daan and Tuyls, Karl and Hennes, Daniel and Kaisers, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={53},
  pages={659--697},
  year={2015},
}

@phdthesis{jakob,
  title={Deep multi-agent reinforcement learning},
  author={Foerster, Jakob N},
  year={2018},
  school={University of Oxford},
}

@inproceedings{comm,
  title={Learning to communicate with deep multi-agent reinforcement learning},
  author={Foerster, Jakob and Assael, Ioannis Alexandros and De Freitas, Nando and Whiteson, Shimon},
  booktitle={Advances in neural information processing systems},
  pages={2137--2145},
  year={2016},
}

@inproceedings{maddpg,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi I and Tamar, Aviv and Harb, Jean and Abbeel, OpenAI Pieter and Mordatch, Igor},
  booktitle={Advances in neural information processing systems},
  pages={6379--6390},
  year={2017},
}

@inproceedings{aac,
  title={Actor-attention-critic for multi-agent reinforcement learning},
  author={Iqbal, Shariq and Sha, Fei},
  booktitle={International Conference on Machine Learning},
  pages={2961--2970},
  year={2019},
  organization={PMLR},
}

@article{rmaddpg,
  title={R-maddpg for partially observable environments and limited communication},
  author={Wang, Rose E and Everett, Michael and How, Jonathan P},
  journal={arXiv preprint arXiv:2002.06684},
  year={2020},
}

@inproceedings{double,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in neural information processing systems},
  pages={2613--2621},
  year={2010},
}

@inproceedings{coordinate,
  title={Reinforcement social learning of coordination in networked cooperative multiagent systems},
  author={Hao, Jianye and Huang, Dongping and Cai, Yi and Leung, Ho-Fung},
  booktitle={AAAI workshop on multiagent interaction without prior coordination (MIPC 2014)},
  year={2014},
}

@inproceedings{coordinatedrl,
  title={Coordinated reinforcement learning},
  author={Guestrin, Carlos and Lagoudakis, Michail and Parr, Ronald},
  booktitle={ICML},
  volume={2},
  pages={227--234},
  year={2002},
}

@article{survey,
  title={A survey on transfer learning for multiagent reinforcement learning systems},
  author={Da Silva, Felipe Leno and Costa, Anna Helena Reali},
  journal={Journal of Artificial Intelligence Research},
  volume={64},
  pages={645--703},
  year={2019},
}

@article{ltc,
  title={From single-agent to multi-agent reinforcement learning: Foundational concepts and methods},
  author={Neto, Gon{\c{c}}alo},
  journal={Learning theory course},
  year={2005},
}

@incollection{gt,
  title={Game theory and multi-agent reinforcement learning},
  author={Now{\'e}, Ann and Vrancx, Peter and De Hauwere, Yann-Micha{\"e}l},
  booktitle={Reinforcement Learning},
  pages={441--470},
  year={2012},
  publisher={Springer},
}

@article{selective,
  title={Multi-agent reinforcement learning: A selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={arXiv preprint arXiv:1911.10635},
  year={2019},
}

@article{cooperative,
  title={Cooperative multi-agent learning: The state of the art},
  author={Panait, Liviu and Luke, Sean},
  journal={Autonomous agents and multi-agent systems},
  volume={11},
  number={3},
  pages={387--434},
  year={2005},
  publisher={Springer},
}

@Article{atari,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  archivePrefix = {arXiv},
  eprint    = {1312.5602},
}

@Article{go,
  added-at = {2016-03-11T14:36:05.000+0100},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  journal = {Nature},
  month = jan,
  number = 7587,
  pages = {484--489},
  publisher = {Nature Publishing Group},
  timestamp = {2016-03-11T14:37:40.000+0100},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  volume = 529,
  year = {2016},
}

@misc{shogi,
    title={Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
    author={Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy Lillicrap and David Silver},
    year={2019},
    eprint={1911.08265},
    archivePrefix={arXiv},
}

@Article{ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Manfred Otto Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  journal={CoRR},
  year={2015},
  volume={abs/1509.02971},
}

@article{ppo,
  added-at = {2019-12-16T18:31:56.000+0100},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal = {CoRR},
  timestamp = {2019-12-18T21:15:59.000+0100},
  title = {Proximal Policy Optimization Algorithms.},
  volume = {abs/1707.06347},
  year = {2017},
}

@misc{SC2,
    title={StarCraft II: A New Challenge for Reinforcement Learning},
    author={Oriol Vinyals and Timo Ewalds and Sergey Bartunov and Petko Georgiev and Alexander Sasha Vezhnevets and Michelle Yeo and Alireza Makhzani and Heinrich Küttler and John Agapiou and Julian Schrittwieser and John Quan and Stephen Gaffney and Stig Petersen and Karen Simonyan and Tom Schaul and Hado van Hasselt and David Silver and Timothy Lillicrap and Kevin Calderone and Paul Keet and Anthony Brunasso and David Lawrence and Anders Ekermo and Jacob Repp and Rodney Tsing},
    year={2017},
    eprint={1708.04782},
    archivePrefix={arXiv},
}

@article{alphastar,
author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John and Jaderberg, Max and Silver, David},
year = {2019},
month = {11},
pages = {},
title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
volume = {575},
journal = {Nature},
}

@article{dec,
author = {Kraemer, Landon and Banerjee, Bikramjit},
year = {2016},
month = {02},
title = {Multi-agent reinforcement learning as a rehearsal for decentralized planning},
volume = {190},
journal = {Neurocomputing},
}

@INPROCEEDINGS{iql,
    author = {Ming Tan},
    title = {Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents},
    booktitle = {In Proceedings of the Tenth International Conference on Machine Learning},
    year = {1993},
}

@misc{coma,
    title={Counterfactual Multi-Agent Policy Gradients},
    author={Jakob Foerster and Gregory Farquhar and Triantafyllos Afouras and Nantas Nardelli and Shimon Whiteson},
    year={2017},
    eprint={1705.08926},
    archivePrefix={arXiv},
}

@inproceedings{vdn,
author = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore},
title = {Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward},
year = {2018},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2085–2087},
series = {AAMAS ’18},
}
  
@inproceedings{qmix,
  title = "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
  author = "Tabish Rashid and Mikayel Samvelyan and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Shimon Whiteson",
  year = "2018",
  booktitle = "ICML 2018: Proceedings of the Thirty-Fifth International Conference on Machine Learning",
}

@misc{surprise,
    title={Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning},
    author={Joshua Achiam and Shankar Sastry},
    year={2017},
    eprint={1703.01732},
    archivePrefix={arXiv},
}

@article{smirl,
  title={SMiRL: Surprise Minimizing RL in Entropic Environments},
  author={Berseth, Glen and Geng, Daniel and Devin, Coline and Jayaraman, Dinesh and Finn, Chelsea and Levine, Sergey},
  year={2019},
}

@misc{mbrl,
    title={Model-Based Reinforcement Learning for Atari},
    author={Lukasz Kaiser and Mohammad Babaeizadeh and Piotr Milos and Blazej Osinski and Roy H Campbell and Konrad Czechowski and Dumitru Erhan and Chelsea Finn and Piotr Kozakowski and Sergey Levine and Afroz Mohiuddin and Ryan Sepassi and George Tucker and Henryk Michalewski},
    year={2019},
    eprint={1903.00374},
    archivePrefix={arXiv},
}

@misc{gen,
    title={Reinforcement Learning Generalization with Surprise Minimization},
    author={Jerry Zikun Chen},
    year={2020},
    eprint={2004.12399},
    archivePrefix={arXiv},
}

@incollection{doubleqlearning,
title = {Double Q-learning},
author = {Hado V. Hasselt},
booktitle = {Advances in Neural Information Processing Systems 23},
year = {2010},
}

@inproceedings{deepdoubleqlearning,
author = {Hasselt, Hado van and Guez, Arthur and Silver, David},
title = {Deep Reinforcement Learning with Double Q-Learning},
year = {2016},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
}

@misc{td3,
    title={Addressing Function Approximation Error in Actor-Critic Methods},
    author={Scott Fujimoto and Herke van Hoof and David Meger},
    year={2018},
    eprint={1802.09477},
    archivePrefix={arXiv},
} 

@inproceedings{
maxmin,
title={Maxmin Q-learning: Controlling the Estimation Bias of Q-learning},
author={Qingfeng Lan and Yangchen Pan and Alona Fyshe and Martha White},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{surpmodeling,
  title={Modeling forms of surprise in artificial agents: empirical and theoretical study of surprise functions},
  author={Macedo, Luis and Reisezein, Rainer and Cardoso, Amilcar},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={26},
  year={2004},
}

@inproceedings{role,
  title={The role of surprise, curiosity and hunger on exploration of unknown environments populated with entities},
  author={Macedo, Luis and Cardoso, Amilcar},
  booktitle={2005 portuguese conference on artificial intelligence},
  year={2005},
}

@misc{wqmix,
    title={Weighted QMIX: Expanding Monotonic Value Function Factorisation},
    author={Tabish Rashid and Gregory Farquhar and Bei Peng and Shimon Whiteson},
    year={2020},
    eprint={2006.10800},
    archivePrefix={arXiv},
}

@misc{smac,
    title={The StarCraft Multi-Agent Challenge},
    author={Mikayel Samvelyan and Tabish Rashid and Christian Schroeder de Witt and Gregory Farquhar and Nantas Nardelli and Tim G. J. Rudner and Chia-Man Hung and Philip H. S. Torr and Jakob Foerster and Shimon Whiteson},
    year={2019},
    eprint={1902.04043},
    archivePrefix={arXiv},
}

@book{rl,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
}

@article {nash,
	author = {Nash, John F.},
	title = {Equilibrium points in n-person games},
	volume = {36},
	number = {1},
	year = {1950},
	publisher = {National Academy of Sciences},
	journal = {Proceedings of the National Academy of Sciences},
}

@inproceedings{a3c,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  year={2016},
}

@article{rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  journal={arXiv preprint arXiv:1710.02298},
  year={2017},
}

@inproceedings{curiosity,
  Author = {Burda, Yuri and
  Edwards, Harri and Pathak, Deepak and
  Storkey, Amos and Darrell, Trevor and
  Efros, Alexei A.},
  Title = {Large-Scale Study of
  Curiosity-Driven Learning},
  Booktitle = {ICLR},
  Year = {2019},
}

@article{exploration,
  title={Efficient exploration in reinforcement learning},
  author={Thrun, Sebastian B},
  year={1992},
  publisher={Citeseer},
}


@article{effectiveexp,
  title={Incentivizing exploration in reinforcement learning with deep predictive models},
  author={Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1507.00814},
  year={2015},
}

@article{statemarginal,
  title={Efficient exploration via state marginal matching},
  author={Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1906.05274},
  year={2019},
}


@incollection{metaexp,
title = {Meta-Reinforcement Learning of Structured Exploration Strategies},
author = {Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems 31},
year = {2018},
}

@inproceedings{hdqn,
  title={Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation},
  author={Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
  booktitle={Advances in neural information processing systems},
  year={2016},
}

@inproceedings{marlsurp,
  title={A survey of consensus problems in multi-agent coordination},
  author={Ren, Wei and Beard, Randal W and Atkins, Ella M},
  booktitle={Proceedings of the 2005, American Control Conference, 2005.},
  year={2005},
}

@article{sac,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018},
}

@inproceedings{duel,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International conference on machine learning},
  year={2016},
}

@article{marlover,
  title={Reducing Overestimation Bias in Multi-Agent Domains Using Double Centralized Critics},
  author={Ackermann, Johannes and Gabler, Volker and Osa, Takayuki and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:1910.01465},
  year={2019},
}

@inproceedings{iqn,
  title={Likelihood Quantile Networks for Coordinating Multi-Agent Reinforcement Learning},
  author={Lyu, Xueguang and Amato, Christopher},
  booktitle={Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  year={2020},
}

@article{twinmix,
  title={Reducing Overestimation in Value Mixing for Cooperative Deep Multi-Agent Reinforcement Learning},
  author={Fu, Zipeng and Zhao, Qingqing and Zhang, Weinan},
  journal={ICAART},
  year={2020},
}

@inproceedings{weightdouble,
  title={Weighted double deep multiagent reinforcement learning in stochastic cooperative environments},
  author={Zheng, Yan and Meng, Zhaopeng and Hao, Jianye and Zhang, Zongzhang},
  booktitle={Pacific Rim international conference on artificial intelligence},
  year={2018},
}

@article{challenges,
  title={Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications},
  author={Nguyen, Thanh Thi and Nguyen, Ngoc Duy and Nahavandi, Saeid},
  journal={IEEE transactions on cybernetics},
  year={2020},
}

@article{ebm,
  title={A tutorial on energy-based learning},
  author={LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, M and Huang, F},
  journal={Predicting structured data},
  volume={1},
  year={2006},
}

@inproceedings{ebmdoc,
  title={Energy-based models in document recognition and computer vision},
  author={LeCun, Yann and Chopra, Sumit and Ranzato, M and Huang, F-J},
  booktitle={Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)},
  volume={1},
  year={2007},
}

@article{energy,
  title={Energy-based models for sparse overcomplete representations},
  author={Teh, Yee Whye and Welling, Max and Osindero, Simon and Hinton, Geoffrey E},
  journal={Journal of Machine Learning Research},
  volume={4},
  year={2003},
}

@book{david,
author = {MacKay, David J. C.},
title = {Information Theory, Inference \& Learning Algorithms},
year = {2002},
publisher = {Cambridge University Press},
}

@article{rlhinton,
  title={Reinforcement learning with factored states and actions},
  author={Sallans, Brian and Hinton, Geoffrey E},
  journal={Journal of Machine Learning Research},
  volume={5},
  year={2004},
}

@article{pgq,
  title={Combining policy gradient and Q-learning},
  author={O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
  journal={arXiv preprint arXiv:1611.01626},
  year={2016},
}

@article{ziebartphd,
  title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
  author={Ziebart, Brian D},
  year={2010},
}

@inproceedings{ziebartinverse,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={AAAI},
  year={2008},
}

@inproceedings{boltzmann,
  title={Learning neural network policies with guided policy search under unknown dynamics},
  author={Levine, Sergey and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  year={2014},
}

@inproceedings{inference,
  title={Robot trajectory optimization using approximate inference},
  author={Toussaint, Marc},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  year={2009},
}

@article{sql,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1702.08165},
  year={2017},
}

@article{equivalence,
  title={Equivalence between policy gradients and soft q-learning},
  author={Schulman, John and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.06440},
  year={2017},
}

@article{marlsql,
  title={Multiagent soft q-learning},
  author={Wei, Ermo and Wicke, Drew and Freelan, David and Luke, Sean},
  journal={arXiv preprint arXiv:1804.09817},
  year={2018},
}

@article{twoplayer,
  title={Balancing two-player stochastic games with soft q-learning},
  author={Grau-Moya, Jordi and Leibfried, Felix and Bou-Ammar, Haitham},
  journal={arXiv preprint arXiv:1802.03216},
  year={2018},
}

@article{hierarchical,
  title={Latent space policies for hierarchical reinforcement learning},
  author={Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1804.02808},
  year={2018},
}

@phdthesis{haarnoja,
  title={Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning},
  author={Haarnoja, Tuomas},
  year={2018},
  school={UC Berkeley},
}

@incollection{overview,
  title={Multi-agent reinforcement learning: An overview},
  author={Bu{\c{s}}oniu, Lucian and Babu{\v{s}}ka, Robert and De Schutter, Bart},
  booktitle={Innovations in multi-agent systems and applications-1},
  year={2010},
}

@article{probabilistic,
  title={Probabilistic recursive reasoning for multi-agent reinforcement learning},
  author={Wen, Ying and Yang, Yaodong and Luo, Rui and Wang, Jun and Pan, Wei},
  journal={arXiv preprint arXiv:1901.09207},
  year={2019},
}

@article{homer,
  title={Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning},
  author={Misra, Dipendra and Henaff, Mikael and Krishnamurthy, Akshay and Langford, John},
  journal={arXiv preprint arXiv:1911.05815},
  year={2019},
}

@article{predator,
  title={Multiagent systems: A survey from a machine learning perspective},
  author={Stone, Peter and Veloso, Manuela},
  journal={Autonomous Robots},
  volume={8},
  year={2000},
}

@inproceedings{survey,
  title={Multi-agent reinforcement learning: A survey},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  booktitle={2006 9th International Conference on Control, Automation, Robotics and Vision},
  year={2006},
}

@inproceedings{mellowmax,
  title={An alternative softmax operator for reinforcement learning},
  author={Asadi, Kavosh and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  year={2017},
}